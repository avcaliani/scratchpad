# Spark - High Order Functions

![#](https://img.shields.io/badge/apache--spark-3.3.1-darkorange.svg?logo=apache-spark&logoColor=white)

**This is an experiment using Spark array functions.**  
In this example I'm using a [Fraudulent Transactions Data](https://www.kaggle.com/datasets/chitwanmanchanda/fraudulent-transactions-data) dataset, so thanks to [Chitwan Manchanda](https://www.kaggle.com/chitwanmanchanda) for sharing his dataset.

## How to use it?

1. Dataset Download  
   Download the dataset from Kaggle, then add the file into the folder `./data/raw/`

2. Execute the Script `spark-submit main.py`

## References

- [Databricks: Higher-order functions](https://docs.databricks.com/optimizations/higher-order-lambda-functions.html)
- [Databricks: Working with Nested Data Using Higher Order Functions in SQL](https://www.databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html)
- [Javadoc: Column](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/Column.html)
